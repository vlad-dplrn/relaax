---

# This is an example of configuration to "run" sample algorithms w/ sample clients
# This sample main purpose to demonstrate wiring of the client and algorithms
# To run this "training" set algorithm/path to relaax algorithm location 'algorithms/sample'
# Start parameter server
# relaax-parameter-server --config sample.yaml
# Start RLX server
# relaax-rlx-server --config sample.yaml
# run client
# python client/sample_exchange.py

environment:
  run: ../environments/DeepMind/main
  level-script: tests/my_map
  rlx-server: localhost:7001

relaax-parameter-server:
  log-level: INFO
  bind: localhost:7000
#  checkpoint-dir: training/checkpoints/sample_fun_lab
#  checkpoint-aws-s3: dl-checkpoints/sample_fun_lab
  checkpoint-time-interval: 60
#  checkpoint-step-interval: 1000
  checkpoints-to-keep: 5
  aws-keys: aws_keys.yaml
  metrics-dir: training/metrics/sample_fun_lab

relaax-rlx-server:
  bind: localhost:7001
  log-level: DEBUG

algorithm:
  name: fun

  action_size: 6                  # action size for given game rom (18 fits ale boxing)
  batch_size: 10                  # local loop size for one episode

  input:
    shape: [84, 84]
    channels: 3
    history: 1
    use_convolutions:
      - n_filters: 16
        filter_size: [8, 8]
        stride: [4, 4]
      - n_filters: 32
        filter_size: [4, 4]
        stride: [2, 2]

  gpu: false                      # to use GPU, set to the True
  lstm: true                      # to use LSTM instead of FF, set to the True
  max_global_step: 1e9            # amount of maximum global steps to pass through the training

  learning_rate: 2e-4             # learning rate
  anneal_step_limit: 2e8          # annealing lr within first 200mil

  entropy_beta: 1e-2              # entropy regularization constant
  worker_gamma: 0.99              # rewards discount factor for workers
  manager_gamma: 0.999            # discount factor for managers

  RMSProp:
    decay: 0.99
    epsilon: 0.1
    gradient_norm_clipping: 40

  # feudal representation
  d: 256    # internal representation size
  k: 16     # output representation size
  h: 10     # number of manager's cores (horizon)
  c: 10     # goal horizon to sum up
  alpha: 1  # alpha for intrinsic reward
